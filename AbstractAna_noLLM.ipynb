{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AbstractAna — no‑LLM fallback\n",
        "\n",
        "This notebook tags arXiv abstracts **without** relying on a local LLM server. It uses a **TF‑IDF n‑gram** extractor to generate concise tags per abstract and writes them to `papers.llm_tags` in the SQLite DB.\n",
        "\n",
        "If later set up an OpenAI‑compatible endpoint (vLLM/LM Studio/Ollama with compat), one can flip the switch to use it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Config ---\n",
        "from pathlib import Path\n",
        "DB_PATH = Path('/Users/wenzheng/Desktop/LLM CS quant/ZZW-LLM/RAGAnalyzer/arxiv.db')  # <-- update to   arxiv.db\n",
        "USE_LLM = False           # set True to attempt OpenAI-compatible server; otherwise fallback TF-IDF\n",
        "OPENAI_BASE_URL = 'http://127.0.0.1:8889/v1'  # change if enable an LLM server\n",
        "OPENAI_MODEL = '/models/Qwen3-8B'\n",
        "MAX_TAGS = 20\n",
        "LIMIT = 200               # None for all\n",
        "print(DB_PATH.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Imports & helpers ---\n",
        "import sqlite3, re, json\n",
        "from typing import List, Tuple\n",
        "import requests\n",
        "\n",
        "def remove_think_tag(text: str) -> str:\n",
        "    return re.sub(r\"<think>.*?</think>\\s*\", \"\", text, flags=re.DOTALL)\n",
        "\n",
        "def ensure_llm_tags_column(con: sqlite3.Connection):\n",
        "    cur = con.cursor()\n",
        "    cur.execute(\"PRAGMA table_info(papers)\")\n",
        "    cols = {row[1] for row in cur.fetchall()}\n",
        "    if 'llm_tags' not in cols:\n",
        "        cur.execute(\"ALTER TABLE papers ADD COLUMN llm_tags TEXT\")\n",
        "        con.commit()\n",
        "        print(\"[init] Added llm_tags TEXT to papers\")\n",
        "    else:\n",
        "        print(\"[init] llm_tags column already present\")\n",
        "\n",
        "def load_papers(con: sqlite3.Connection, limit: int | None = None) -> List[Tuple[str, str, str]]:\n",
        "    cur = con.cursor()\n",
        "    q = \"SELECT id, title, summary FROM papers WHERE summary IS NOT NULL AND trim(summary) <> '' AND (llm_tags IS NULL OR trim(llm_tags) = '') ORDER BY published DESC\"\n",
        "    if limit:\n",
        "        q += f\" LIMIT {int(limit)}\"\n",
        "    cur.execute(q)\n",
        "    return cur.fetchall()\n",
        "\n",
        "def try_ping_llm(base_url: str) -> bool:\n",
        "    try:\n",
        "        r = requests.get(base_url.rstrip('/') + '/models', timeout=2)\n",
        "        return r.ok\n",
        "    except Exception:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF‑IDF tagger (fallback)\n",
        "Extract top n‑gram phrases (1–3) per abstract using a TF‑IDF vectorizer. This is a quick, dependency‑light alternative to LLM tagging.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Simple stopword list; can be extended\n",
        "STOPWORDS = set('''a an the and or but if then else for while to of in on at by from with without within upon about into over under above below between among across through during before after\n",
        "is are was were be been being have has had do does did can could should would may might must will shall not no nor so than too very just also more most many much few several each every per via using use used\n",
        "we our us you your they their them he she it its this that these those as such one two three four five six seven eight nine ten et al etal etc i ii iii iv v vi vii viii ix x'.split())\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "def tfidf_tags(docs: list[str], max_tags: int = 20) -> list[list[str]]:\n",
        "    # Vectorize with 1-3 grams, basic token pattern; English stopwords via custom list\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        ngram_range=(1,3),\n",
        "        lowercase=True,\n",
        "        token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z0-9_\\-]{1,}\\b\",\n",
        "        min_df=2,          # ignore ultra-rare terms (tune if corpus is small)\n",
        "        max_df=0.9,\n",
        "    )\n",
        "    clean_docs = [normalize_text(d) for d in docs]\n",
        "    X = vectorizer.fit_transform(clean_docs)\n",
        "    vocab = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "    results: list[list[str]] = []\n",
        "    for i in range(X.shape[0]):\n",
        "        row = X.getrow(i)\n",
        "        if row.nnz == 0:\n",
        "            results.append([])\n",
        "            continue\n",
        "        idx = row.indices\n",
        "        vals = row.data\n",
        "        order = np.argsort(-vals)  # descending by tf-idf\n",
        "        tags = []\n",
        "        seen = set()\n",
        "        for j in order:\n",
        "            term = vocab[idx[j]]\n",
        "            # Basic stopword filtering: skip if term is mostly stopwords or too short\n",
        "            words = term.split()\n",
        "            if all(w in STOPWORDS or len(w) <= 2 for w in words):\n",
        "                continue\n",
        "            key = term.lower()\n",
        "            if key in seen:\n",
        "                continue\n",
        "            tags.append(term)\n",
        "            seen.add(key)\n",
        "            if len(tags) >= max_tags:\n",
        "                break\n",
        "        results.append(tags)\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run tagging\n",
        "If `USE_LLM=True` **and** the server responds at `OPENAI_BASE_URL`, the notebook would call the LLM. Otherwise, it falls back to TF‑IDF.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "\n",
        "con = sqlite3.connect(DB_PATH)\n",
        "ensure_llm_tags_column(con)\n",
        "rows = load_papers(con, limit=LIMIT)\n",
        "print(f\"Loaded {len(rows)} papers to tag\")\n",
        "\n",
        "ids = [r[0] for r in rows]\n",
        "titles = [r[1] for r in rows]\n",
        "abstracts = [r[2] for r in rows]\n",
        "\n",
        "use_llm_now = False\n",
        "if USE_LLM:\n",
        "    use_llm_now = try_ping_llm(OPENAI_BASE_URL)\n",
        "print(f\"LLM available: {use_llm_now}\")\n",
        "\n",
        "if not rows:\n",
        "    print(\"Nothing to tag.\")\n",
        "else:\n",
        "    if use_llm_now:\n",
        "        raise NotImplementedError(\"LLM path disabled in this fallback notebook. Set USE_LLM=False or use the cleaned LLM notebook.\")\n",
        "    else:\n",
        "        all_tags = tfidf_tags(abstracts, max_tags=MAX_TAGS)\n",
        "        cur = con.cursor()\n",
        "        for pid, tags in zip(ids, all_tags):\n",
        "            cur.execute(\"UPDATE papers SET llm_tags = ? WHERE id = ?\", (\", \".join(tags), pid))\n",
        "        con.commit()\n",
        "        print(f\"[write] Wrote tags for {len(ids)} papers using TF-IDF fallback.\")\n",
        "\n",
        "con.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot: top tags\n",
        "Quick view of the most frequent tags found in `papers.llm_tags`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sqlite3, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "con = sqlite3.connect(DB_PATH)\n",
        "df = pd.read_sql_query(\"SELECT llm_tags FROM papers WHERE llm_tags IS NOT NULL AND trim(llm_tags) <> ''\", con)\n",
        "con.close()\n",
        "\n",
        "if df.empty:\n",
        "    print(\"No tags yet. Run the tagging cell first.\")\n",
        "else:\n",
        "    tags = (\n",
        "        df['llm_tags']\n",
        "        .str.split(',')\n",
        "        .explode()\n",
        "        .str.strip()\n",
        "    )\n",
        "    top = tags.value_counts().head(30)\n",
        "    plt.figure()\n",
        "    top.sort_values().plot(kind='barh')\n",
        "    plt.title('Top tags (TF-IDF fallback)')\n",
        "    plt.xlabel('Count')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
