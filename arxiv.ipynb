{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two pipelines builder -- arXiv paper scraper & Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. arXiv paper scraper (RAG-focused):\n",
    "    - Search arXiv in all CS categories (cat:cs.*) for papers that mention “Retrieval Augmented Generation”.\n",
    "    - Iterate results (up to a large max_results) and download PDFs into a local folder, skipping anything already downloaded (tracked by a text file).\n",
    "    - Print progress and robustly handle HTTP/network errors.\n",
    "\n",
    "\n",
    "2. Feed → JSONL converter (with granary + BeautifulSoup):\n",
    "    - Fetch one or more RSS/Atom feeds.\n",
    "\n",
    "    - Convert them to **JSON Feed** format using **granary**.\n",
    "\n",
    "    - Strip HTML to plain text using BeautifulSoup.\n",
    "\n",
    "    - Append each item as one line in feed.jsonl on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1). arXiv scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cl/8sl72jd516b7g3dx0s3n1vd80000gn/T/ipykernel_47744/1743592317.py:13: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  print(len(list(search.results())))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cl/8sl72jd516b7g3dx0s3n1vd80000gn/T/ipykernel_47744/1743592317.py:16: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: End-to-End Trainable Retrieval-Augmented Generation for Relation Extraction\n",
      "Title: DuetRAG: Collaborative Retrieval-Augmented Generation\n",
      "Title: Meta-prompting Optimized Retrieval-augmented Generation\n",
      "Title: ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation\n",
      "Title: A Retrieval-Augmented Generation Framework for Academic Literature Navigation in Data Science\n",
      "Title: Benchmarking Large Language Models in Retrieval-Augmented Generation\n",
      "Title: Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts\n",
      "Title: Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education\n",
      "Title: Harnessing Retrieval-Augmented Generation (RAG) for Uncovering Knowledge Gaps\n",
      "Title: Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models\n",
      "Title: Retrieval-Augmented Generation for Generative Artificial Intelligence in Medicine\n",
      "Title: Towards Retrieval Augmented Generation over Large Video Libraries\n",
      "Title: A Simple Architecture for Enterprise Large Language Model Applications based on Role based security and Clearance Levels using Retrieval-Augmented Generation or Mixture of Experts\n",
      "Title: PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents\n",
      "Title: SeeSay: An Assistive Device for the Visually Impaired Using Retrieval Augmented Generation\n",
      "Title: AutoRAG: Automated Framework for optimization of Retrieval Augmented Generation Pipeline\n",
      "Title: BioRAGent: A Retrieval-Augmented Generation System for Showcasing Generative Query Expansion and Domain-Specific Search for Scientific Q&A\n",
      "Title: RAG-Verus: Repository-Level Program Verification with LLMs using Retrieval Augmented Generation\n",
      "Title: ParetoRAG: Leveraging Sentence-Context Attention for Robust and Efficient Retrieval-Augmented Generation\n",
      "Title: FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA\n",
      "Title: Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation\n",
      "Title: Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented Generation Systems\n",
      "Title: CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through Self-Training\n",
      "Title: Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3\n",
      "Title: Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation\n",
      "Title: GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation\n",
      "Title: Ragas: Automated Evaluation of Retrieval Augmented Generation\n",
      "Title: Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation\n",
      "Title: Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning\n",
      "Title: Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering\n",
      "Title: RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation\n",
      "Title: GRAG: Graph Retrieval-Augmented Generation\n",
      "Title: ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator\n",
      "Title: An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation\n",
      "Title: R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation\n",
      "Title: Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation\n",
      "Title: RAMO: Retrieval-Augmented Generation for Enhancing MOOCs Recommendations\n",
      "Title: The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented Generation for Healthcare QA\n",
      "Title: CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmented Generation in Fact-Checking\n",
      "Title: AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support\n",
      "Title: Self-adaptive Multimodal Retrieval-Augmented Generation\n",
      "Title: FRAG: Toward Federated Vector Database Management for Collaborative and Secure Retrieval-Augmented Generation\n",
      "Title: E2E-AFG: An End-to-End Model with Adaptive Filtering for Retrieval-Augmented Generation\n",
      "Title: Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering\n",
      "Title: Context Awareness Gate For Retrieval Augmented Generation\n",
      "Title: VISA: Retrieval Augmented Generation with Visual Source Attribution\n",
      "Title: SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven Retrieval-Augmented Generation\n",
      "Title: Unveiling the Potential of Multimodal Retrieval Augmented Generation with Planning\n",
      "Title: VideoRAG: Retrieval-Augmented Generation with Extreme Long-Context Videos\n",
      "Title: Knowledge Graph-Guided Retrieval Augmented Generation\n",
      "Title: SpeechT-RAG: Reliable Depression Detection in LLMs with Retrieval-Augmented Generation Using Speech Timing Information\n",
      "Title: DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue\n",
      "Title: WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models\n",
      "Title: Neural Graph Matching Improves Retrieval Augmented Generation in Molecular Machine Learning\n",
      "Title: RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving\n",
      "Title: A Survey of Multimodal Retrieval-Augmented Generation\n",
      "Title: Knowledge Graph-extended Retrieval Augmented Generation for Question Answering\n",
      "Title: CDF-RAG: Causal Dynamic Feedback for Adaptive Retrieval-Augmented Generation\n",
      "Title: Distributed Retrieval-Augmented Generation\n",
      "Title: SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation\n",
      "Title: CReSt: A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex Reasoning over Structured Documents\n",
      "Title: Federated Retrieval-Augmented Generation: A Systematic Mapping Study\n",
      "Title: MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering\n",
      "Title: Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory\n",
      "Title: Active Retrieval Augmented Generation\n",
      "Title: Prompt Generate Train (PGT): Few-shot Domain Adaption of Retrieval Augmented Generation Models for Open Book Question-Answering\n",
      "Title: How to Build an Adaptive AI Tutor for Any Course Using Knowledge Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)\n",
      "Title: Corrective Retrieval Augmented Generation\n",
      "Title: Benchmarking Retrieval-Augmented Generation for Medicine\n",
      "Title: DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models\n",
      "Title: xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token\n",
      "Title: DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented Generation for Question-Answering\n",
      "Title: Learning to Explore and Select for Coverage-Conditioned Retrieval-Augmented Generation\n",
      "Title: Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework for Medical Applications\n",
      "Title: Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support\n",
      "Title: RuleRAG: Rule-Guided Retrieval-Augmented Generation with Language Models for Question Answering\n",
      "Title: CORAG: A Cost-Constrained Retrieval Optimization System for Retrieval-Augmented Generation\n",
      "Title: LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology Report Generation\n",
      "Title: A Collaborative Multi-Agent Approach to Retrieval-Augmented Generation Across Diverse Data\n",
      "Title: C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System\n",
      "Title: MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation\n",
      "Title: Advancing Retrieval-Augmented Generation for Persian: Development of Language Models, Comprehensive Benchmarks, and Best Practices for Optimization\n",
      "Title: Retrieval-Augmented Generation by Evidence Retroactivity in LLMs\n",
      "Title: Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG\n",
      "Title: GEC-RAG: Improving Generative Error Correction via Retrieval-Augmented Generation for Automatic Speech Recognition Systems\n",
      "Title: A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models\n",
      "Title: Parametric Retrieval Augmented Generation\n",
      "Title: Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented Generation\n",
      "Title: Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts\n",
      "Title: A Multi-agent Onboarding Assistant based on Large Language Models, Retrieval Augmented Generation, and Chain-of-Thought\n",
      "Title: MMKB-RAG: A Multi-Modal Knowledge-Based Retrieval-Augmented Generation Framework\n",
      "Title: HeteRAG: A Heterogeneous Retrieval-augmented Generation Framework with Decoupled Knowledge Representations\n",
      "Title: Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented Generation\n",
      "Title: POLYRAG: Integrating Polyviews into Retrieval-Augmented Generation for Medical Applications\n",
      "Title: Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation\n",
      "Title: Retrieval Augmented Generation Evaluation for Health Documents\n",
      "Title: Optimizing Retrieval Augmented Generation for Object Constraint Language\n",
      "Title: ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation\n",
      "Title: HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation\n",
      "Title: KERAG_R: Knowledge-Enhanced Retrieval-Augmented Generation for Recommendation\n"
     ]
    }
   ],
   "source": [
    "# test the search string\n",
    "import arxiv\n",
    "\n",
    "# Define the search query with a category filter for Computer Science\n",
    "query = '\"Retrieval Augmented Generation\"'\n",
    "\n",
    "# Perform the search \n",
    "search = arxiv.Search(\n",
    "    query=query,\n",
    "    max_results=100,  # Limit the number of results\n",
    "    sort_by=arxiv.SortCriterion.Relevance  # Sort by relevance\n",
    ")\n",
    "print(len(list(search.results())))\n",
    "\n",
    "# Display the results\n",
    "for result in search.results():\n",
    "    print(f\"Title: {result.title}\")\n",
    "    #print(f\"Authors: {', '.join(author.name for author in result.authors)}\")\n",
    "    #print(f\"Published: {result.published}\")\n",
    "    #print(f\"Summary: {result.summary}\")\n",
    "    #print(f\"PDF Link: {result.pdf_url}\")\n",
    "    #print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cl/8sl72jd516b7g3dx0s3n1vd80000gn/T/ipykernel_47744/3548692113.py:21: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  currentlist=search.results()\n",
      "/var/folders/cl/8sl72jd516b7g3dx0s3n1vd80000gn/T/ipykernel_47744/3548692113.py:22: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  print(len(list(search.results())))\n"
     ]
    },
    {
     "ename": "UnexpectedEmptyPageError",
     "evalue": "Page of results was unexpectedly empty (https://export.arxiv.org/api/query?search_query=cat%3Acs.%2A+AND+%22Retrieval+Augmented+Generation%22&id_list=&sortBy=relevance&sortOrder=descending&start=100&max_results=100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedEmptyPageError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/arxiv/__init__.py:648\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[0;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    650\u001b[0m     HTTPError,\n\u001b[1;32m    651\u001b[0m     UnexpectedEmptyPageError,\n\u001b[1;32m    652\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[1;32m    653\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/arxiv/__init__.py:689\u001b[0m, in \u001b[0;36mClient.__try_parse_feed\u001b[0;34m(self, url, first_page, try_index)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(feed\u001b[38;5;241m.\u001b[39mentries) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m first_page:\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedEmptyPageError(url, try_index, feed)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feed\u001b[38;5;241m.\u001b[39mbozo:\n",
      "\u001b[0;31mUnexpectedEmptyPageError\u001b[0m: Page of results was unexpectedly empty (https://export.arxiv.org/api/query?search_query=cat%3Acs.%2A+AND+%22Retrieval+Augmented+Generation%22&id_list=&sortBy=relevance&sortOrder=descending&start=100&max_results=100)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnexpectedEmptyPageError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/arxiv/__init__.py:648\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[0;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    650\u001b[0m     HTTPError,\n\u001b[1;32m    651\u001b[0m     UnexpectedEmptyPageError,\n\u001b[1;32m    652\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[1;32m    653\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/arxiv/__init__.py:689\u001b[0m, in \u001b[0;36mClient.__try_parse_feed\u001b[0;34m(self, url, first_page, try_index)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(feed\u001b[38;5;241m.\u001b[39mentries) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m first_page:\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedEmptyPageError(url, try_index, feed)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feed\u001b[38;5;241m.\u001b[39mbozo:\n",
      "\u001b[0;31mUnexpectedEmptyPageError\u001b[0m: Page of results was unexpectedly empty (https://export.arxiv.org/api/query?search_query=cat%3Acs.%2A+AND+%22Retrieval+Augmented+Generation%22&id_list=&sortBy=relevance&sortOrder=descending&start=100&max_results=100)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnexpectedEmptyPageError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/arxiv/__init__.py:648\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[0;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    650\u001b[0m     HTTPError,\n\u001b[1;32m    651\u001b[0m     UnexpectedEmptyPageError,\n\u001b[1;32m    652\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[1;32m    653\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/arxiv/__init__.py:689\u001b[0m, in \u001b[0;36mClient.__try_parse_feed\u001b[0;34m(self, url, first_page, try_index)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(feed\u001b[38;5;241m.\u001b[39mentries) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m first_page:\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedEmptyPageError(url, try_index, feed)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feed\u001b[38;5;241m.\u001b[39mbozo:\n",
      "\u001b[0;31mUnexpectedEmptyPageError\u001b[0m: Page of results was unexpectedly empty (https://export.arxiv.org/api/query?search_query=cat%3Acs.%2A+AND+%22Retrieval+Augmented+Generation%22&id_list=&sortBy=relevance&sortOrder=descending&start=100&max_results=100)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnexpectedEmptyPageError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m\n\u001b[1;32m     15\u001b[0m search \u001b[38;5;241m=\u001b[39m arxiv\u001b[38;5;241m.\u001b[39mSearch(\n\u001b[1;32m     16\u001b[0m     query\u001b[38;5;241m=\u001b[39mquery,\n\u001b[1;32m     17\u001b[0m     max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2500\u001b[39m,  \u001b[38;5;66;03m# Limit the number of results\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     sort_by\u001b[38;5;241m=\u001b[39marxiv\u001b[38;5;241m.\u001b[39mSortCriterion\u001b[38;5;241m.\u001b[39mRelevance  \u001b[38;5;66;03m# Sort by relevance\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m currentlist\u001b[38;5;241m=\u001b[39msearch\u001b[38;5;241m.\u001b[39mresults()\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     23\u001b[0m download_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/wenzheng/Desktop/LLM CS quant/ZZW-LLM/RAGAnalyzer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m downloadeded_list_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./downloadeded_list\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/arxiv/__init__.py:622\u001b[0m, in \u001b[0;36mClient._results\u001b[0;34m(self, search, offset)\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    621\u001b[0m page_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_url(search, offset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_size)\n\u001b[0;32m--> 622\u001b[0m feed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/arxiv/__init__.py:656\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[0;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _try_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_retries:\n\u001b[1;32m    655\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot error (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[0;32m--> 656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_try_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiving up (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/arxiv/__init__.py:656\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[0;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _try_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_retries:\n\u001b[1;32m    655\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot error (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[0;32m--> 656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_try_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiving up (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/arxiv/__init__.py:656\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[0;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _try_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_retries:\n\u001b[1;32m    655\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot error (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[0;32m--> 656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_try_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiving up (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/arxiv/__init__.py:658\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[0;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_feed(url, first_page\u001b[38;5;241m=\u001b[39mfirst_page, _try_index\u001b[38;5;241m=\u001b[39m_try_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    657\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiving up (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/arxiv/__init__.py:648\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[0;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03mFetches the specified URL and parses it with feedparser.\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03mIf a request fails or is unexpectedly empty, retries the request up to\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m`self.num_retries` times.\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    650\u001b[0m     HTTPError,\n\u001b[1;32m    651\u001b[0m     UnexpectedEmptyPageError,\n\u001b[1;32m    652\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[1;32m    653\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _try_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_retries:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.11/site-packages/arxiv/__init__.py:689\u001b[0m, in \u001b[0;36mClient.__try_parse_feed\u001b[0;34m(self, url, first_page, try_index)\u001b[0m\n\u001b[1;32m    687\u001b[0m feed \u001b[38;5;241m=\u001b[39m feedparser\u001b[38;5;241m.\u001b[39mparse(resp\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(feed\u001b[38;5;241m.\u001b[39mentries) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m first_page:\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedEmptyPageError(url, try_index, feed)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feed\u001b[38;5;241m.\u001b[39mbozo:\n\u001b[1;32m    692\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBozo feed; consider handling: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    694\u001b[0m         feed\u001b[38;5;241m.\u001b[39mbozo_exception \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbozo_exception\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m feed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    695\u001b[0m     )\n",
      "\u001b[0;31mUnexpectedEmptyPageError\u001b[0m: Page of results was unexpectedly empty (https://export.arxiv.org/api/query?search_query=cat%3Acs.%2A+AND+%22Retrieval+Augmented+Generation%22&id_list=&sortBy=relevance&sortOrder=descending&start=100&max_results=100)"
     ]
    }
   ],
   "source": [
    "# use in rag env\n",
    "\n",
    "import arxiv\n",
    "import os\n",
    "import requests\n",
    "from requests.exceptions import HTTPError, RequestException\n",
    "import urllib.error\n",
    "\n",
    "\n",
    "# Define the search query with a category filter for Computer Science\n",
    "query = 'cat:cs.* AND \"Retrieval Augmented Generation\"'\n",
    "#query = 'ti:\"Retrieval-Augmented Generation\"'\n",
    "\n",
    "# Perform the search\n",
    "search = arxiv.Search(\n",
    "    query=query,\n",
    "    max_results=100,  # Limit the number of results\n",
    "    sort_by=arxiv.SortCriterion.Relevance  # Sort by relevance\n",
    ")\n",
    "\n",
    "currentlist=search.results()\n",
    "print(len(list(search.results())))\n",
    "download_directory = \"/Users/wenzheng/Desktop/LLM CS quant/ZZW-LLM/RAGAnalyzer\"\n",
    "downloadeded_list_path = \"./downloadeded_list\"\n",
    "\n",
    "from itertools import islice\n",
    "start_index = 0\n",
    "currentlist = islice(search.results(), start_index, None)\n",
    "\n",
    "\n",
    "# Check if the file \"files_to_download.txt\" exists\n",
    " # Ensure the download directory exists\n",
    "os.makedirs(download_directory, exist_ok=True)\n",
    "\n",
    "# Check if the downloaded list file exists, if not, create it\n",
    "if not os.path.exists(downloadeded_list_path):\n",
    "    with open(downloadeded_list_path, 'w') as f:\n",
    "        pass  # Create an empty file\n",
    "\n",
    "# Read the downloaded list into a set for quick lookup\n",
    "with open(downloadeded_list_path, 'r') as f:\n",
    "    downloaded_list = set(line.strip() for line in f)\n",
    "count=0\n",
    "\n",
    "\n",
    "\n",
    "# Process the search results\n",
    "for result in currentlist:\n",
    "    paper_id = result.entry_id.split('/')[-1]\n",
    "    if paper_id in downloaded_list:\n",
    "        print(f\"Paper already downloaded, skipping: {result.title}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Download the paper\n",
    "        pdf_filename = f\"{paper_id}.pdf\"\n",
    "        pdf_path = os.path.join(download_directory, pdf_filename)\n",
    "        result.download_pdf(dirpath=download_directory)\n",
    "        print(f\"Downloaded: {pdf_filename}\")\n",
    "\n",
    "        # Add the paper ID to the downloaded list\n",
    "        with open(downloadeded_list_path, 'a') as f:\n",
    "            f.write(paper_id + '\\n')\n",
    "        downloaded_list.add(paper_id)\n",
    "        count += 1\n",
    "        print(\"-\" * 80 + str(count))\n",
    "    except (FileNotFoundError, HTTPError, RequestException, urllib.error.HTTPError) as e:\n",
    "        print(f\"Error downloading {result.title}: {e}\")\n",
    "        print(\"Skipping to the next paper.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error downloading {result.title}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2). Json feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'granary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgranary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jsonfeed, rss\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'granary'"
     ]
    }
   ],
   "source": [
    "# I was recently working on a project to plot historical trends showing mentions of a keyword in a given field of interest.\n",
    "#  For this project, I needed to retrieve abstracts from specific categories (i.e. cs.CV) on Arxiv over periods of time (i.e. the last 180 days). \n",
    "# I wrote a script in Python to do this:\n",
    "\n",
    "#set PYTHONUTF8=1\n",
    "#pip install granary\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from granary import jsonfeed, rss\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "days = 10\n",
    "day_timestamps = [(datetime.now() - timedelta(days=i)).strftime('%Y%m%d') for i in range(1, days)]\n",
    "\n",
    "BASE_URL = 'http://export.arxiv.org/api/query?search_query=cat:cs.CV+AND+submittedDate:[{day}0000+TO+{day}2359]&max_results=1000'\n",
    "\n",
    "feeds = [BASE_URL.format(day=date) for date in day_timestamps]\n",
    "\n",
    "for feed in tqdm(feeds):\n",
    "    try:\n",
    "        resp = requests.get(\n",
    "            feed, headers={\"User-Agent\": \"arxiv-poll\"}\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "    except requests.RequestException:\n",
    "        print(\"Failed to fetch\", feed)\n",
    "        continue\n",
    "\n",
    "    activities = jsonfeed.activities_to_jsonfeed(rss.to_activities(resp.text))\n",
    "\n",
    "    print(\"Fetched\", feed, \"with\", len(activities[\"items\"]), \"activities using feed type\", content_type)\n",
    "\n",
    "    activities[\"items\"] = [\n",
    "        {\n",
    "            **activity,\n",
    "            \"content_html\": BeautifulSoup(activity[\"content_html\"], \"html.parser\").get_text()\n",
    "        }\n",
    "        for activity in activities[\"items\"]\n",
    "    ]\n",
    "\n",
    "    with open(\"feed.jsonl\", \"a+\") as f:\n",
    "        for activity in activities[\"items\"]:\n",
    "            f.write(json.dumps(activity) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
