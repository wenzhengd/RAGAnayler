{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AbstractAna (cleaned)\n",
        "\n",
        "Refactored to work with the SQLite database produced by **json2db_refactored.ipynb**.\n",
        "\n",
        "### What this notebook does\n",
        "1. **Config**: set DB path, vLLM/OpenAI-compatible endpoint, and model name.\n",
        "2. **DB setup**: connect to SQLite and ensure a `llm_tags` column exists on `papers`.\n",
        "3. **Batch tagging**: read abstracts, generate 10–20 tags per paper with a local LLM, and write back to DB.\n",
        "4. **Plotting**: quick top-tags frequency plot.\n",
        "\n",
        "**Notes**:\n",
        "- The new SQLite schema (from `json2db_refactored.ipynb`) typically has tables: `papers(id, arxiv_id, version, title, summary, published, year, month)`, `authors(paper_id, position, author)`, and an FTS table.\n",
        "- If you need arXiv categories (e.g., `cs.CV`), we should extend the importer to store them. For now, this notebook works *without* categories.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Config ---\n",
        "from pathlib import Path\n",
        "DB_PATH = Path('/Users/wenzheng/Desktop/LLM CS quant/ZZW-LLM/RAGAnalyzer/arxiv.db')  # <-- update if needed\n",
        "OPENAI_BASE_URL = 'http://localhost:8889/v1'  # vLLM/OpenAI-compatible endpoint\n",
        "OPENAI_MODEL = '/models/Qwen3-8B'             # your local model name on vLLM\n",
        "BATCH_SIZE = 8                                 # how many abstracts per request\n",
        "MAX_TAGS = 20                                   # cap tags per abstract\n",
        "DRY_RUN = False                                 # True → do not write to DB\n",
        "LIMIT = 200                                     # process at most this many papers (None for all)\n",
        "print(DB_PATH.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Imports ---\n",
        "import sqlite3, re, aiohttp, asyncio, json\n",
        "from typing import List, Tuple\n",
        "\n",
        "def remove_think_tag(text: str) -> str:\n",
        "    return re.sub(r\"<think>.*?</think>\\s*\", \"\", text, flags=re.DOTALL)\n",
        "\n",
        "def ensure_llm_tags_column(con: sqlite3.Connection):\n",
        "    cur = con.cursor()\n",
        "    cur.execute(\"PRAGMA table_info(papers)\")\n",
        "    cols = {row[1] for row in cur.fetchall()}\n",
        "    if 'llm_tags' not in cols:\n",
        "        cur.execute(\"ALTER TABLE papers ADD COLUMN llm_tags TEXT\")\n",
        "        con.commit()\n",
        "        print(\"[init] Added llm_tags TEXT to papers\")\n",
        "    else:\n",
        "        print(\"[init] llm_tags column already present\")\n",
        "\n",
        "def load_papers(con: sqlite3.Connection, limit: int | None = None) -> List[Tuple[str, str]]:\n",
        "    # Returns list of (id, summary) for papers without llm_tags and with non-empty summary\n",
        "    cur = con.cursor()\n",
        "    q = \"SELECT id, summary FROM papers WHERE summary IS NOT NULL AND trim(summary) <> '' AND (llm_tags IS NULL OR trim(llm_tags) = '') ORDER BY published DESC\"\n",
        "    if limit:\n",
        "        q += f\" LIMIT {int(limit)}\"\n",
        "    cur.execute(q)\n",
        "    return cur.fetchall()\n",
        "\n",
        "def parse_tags(text: str, max_tags: int = 20) -> list:\n",
        "    # Accept either comma-separated or line-separated tags\n",
        "    text = remove_think_tag(text)\n",
        "    parts = re.split(r\"[,\\n]\", text)\n",
        "    tags = []\n",
        "    for p in parts:\n",
        "        t = p.strip().strip('-•*').strip()\n",
        "        if t:\n",
        "            tags.append(t)\n",
        "    # Dedup while preserving order\n",
        "    seen = set()\n",
        "    uniq = []\n",
        "    for t in tags:\n",
        "        if t.lower() not in seen:\n",
        "            uniq.append(t)\n",
        "            seen.add(t.lower())\n",
        "    return uniq[:max_tags]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Async tagging with local vLLM ---\n",
        "async def tag_batch(session: aiohttp.ClientSession, items: list[tuple[str,str]]) -> list[tuple[str, list[str]]]:\n",
        "    \"\"\"\n",
        "    items: list of (paper_id, abstract)\n",
        "    returns: list of (paper_id, tags)\n",
        "    \"\"\"\n",
        "    # Build one prompt that includes multiple abstracts, to reduce overhead\n",
        "    prompt_parts = [\n",
        "        \"You are an expert curator. For each abstract, produce 10–20 concise tags (comma-separated), no preface, no numbering.\\n\",\n",
        "        \"Keep tags as simple phrases (2–4 words). Avoid duplicates and generic terms like 'paper', 'study'.\\n\",\n",
        "    ]\n",
        "    for idx, (pid, abs_text) in enumerate(items, 1):\n",
        "        prompt_parts.append(f\"\\n[#{idx}] ABSTRACT:\\n{abs_text}\\nTAGS:\")\n",
        "    user_prompt = \"\".join(prompt_parts)\n",
        "\n",
        "    payload = {\n",
        "        \"model\": OPENAI_MODEL,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"Return only the tags for each abstract in order, separated by new lines; one abstract per line.\"},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        \"temperature\": 0.2,\n",
        "        \"max_tokens\": 1024,\n",
        "    }\n",
        "    url = f\"{OPENAI_BASE_URL}/chat/completions\"\n",
        "    async with session.post(url, json=payload, timeout=aiohttp.ClientTimeout(total=None)) as r:\n",
        "        r.raise_for_status()\n",
        "        data = await r.json()\n",
        "    content = data[\"choices\"][0][\"message\"][\"content\"]\n",
        "    content = remove_think_tag(content)\n",
        "    lines = [ln.strip() for ln in content.splitlines() if ln.strip()]\n",
        "    out: list[tuple[str, list[str]]] = []\n",
        "    for (pid, _), line in zip(items, lines):\n",
        "        out.append((pid, parse_tags(line, MAX_TAGS)))\n",
        "    # If model returned fewer lines than inputs, pad empties\n",
        "    while len(out) < len(items):\n",
        "        pid, _ = items[len(out)]\n",
        "        out.append((pid, []))\n",
        "    return out\n",
        "\n",
        "async def process_all(papers: list[tuple[str,str]]):\n",
        "    results: list[tuple[str, list[str]]] = []\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        for i in range(0, len(papers), BATCH_SIZE):\n",
        "            batch = papers[i:i+BATCH_SIZE]\n",
        "            tagged = await tag_batch(session, batch)\n",
        "            results.extend(tagged)\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Run tagging ---\n",
        "con = sqlite3.connect(DB_PATH)\n",
        "ensure_llm_tags_column(con)\n",
        "papers = load_papers(con, limit=LIMIT)\n",
        "print(f\"Loaded {len(papers)} papers to tag\")\n",
        "if papers:\n",
        "    tagged = asyncio.run(process_all(papers))\n",
        "    print(f\"Tagged {len(tagged)} papers\")\n",
        "    if not DRY_RUN:\n",
        "        cur = con.cursor()\n",
        "        for pid, tags in tagged:\n",
        "            cur.execute(\"UPDATE papers SET llm_tags = ? WHERE id = ?\", (\", \".join(tags), pid))\n",
        "        con.commit()\n",
        "        print(\"[write] Tags stored in DB\")\n",
        "    else:\n",
        "        print(\"[dry-run] Skipped writing tags to DB\")\n",
        "else:\n",
        "    print(\"Nothing to tag.\")\n",
        "con.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot: top tags\n",
        "Simple exploratory plot of the most frequent tags in the `llm_tags` column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Plot top tags ---\n",
        "import sqlite3, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "con = sqlite3.connect(DB_PATH)\n",
        "df = pd.read_sql_query(\"SELECT llm_tags FROM papers WHERE llm_tags IS NOT NULL AND trim(llm_tags) <> ''\", con)\n",
        "con.close()\n",
        "\n",
        "if df.empty:\n",
        "    print(\"No tags yet. Run the tagging cell first.\")\n",
        "else:\n",
        "    # explode comma-separated tags\n",
        "    tags = (\n",
        "        df['llm_tags']\n",
        "        .str.split(',')\n",
        "        .explode()\n",
        "        .str.strip()\n",
        "    )\n",
        "    top = tags.value_counts().head(30)\n",
        "    plt.figure()\n",
        "    top.sort_values().plot(kind='barh')\n",
        "    plt.title('Top tags')\n",
        "    plt.xlabel('Count')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}