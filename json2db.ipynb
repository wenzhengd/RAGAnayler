{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed48d2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"0704.0001\",\n",
      "  \"submitter\": \"Pavel Nadolsky\",\n",
      "  \"authors\": \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\",\n",
      "  \"title\": \"Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies\",\n",
      "  \"comments\": \"37 pages, 15 figures; published version\",\n",
      "  \"journal-ref\": \"Phys.Rev.D76:013009,2007\",\n",
      "  \"doi\": \"10.1103/PhysRevD.76.013009\",\n",
      "  \"report-no\": \"ANL-HEP-PR-07-12\",\n",
      "  \"categories\": \"hep-ph\",\n",
      "  \"license\": null,\n",
      "  \"abstract\": \"  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n\",\n",
      "  \"versions\": [\n",
      "    {\n",
      "      \"version\": \"v1\",\n",
      "      \"created\": \"Mon, 2 Apr 2007 19:18:42 GMT\"\n",
      "    },\n",
      "    {\n",
      "      \"version\": \"v2\",\n",
      "      \"created\": \"Tue, 24 Jul 2007 20:10:27 GMT\"\n",
      "    }\n",
      "  ],\n",
      "  \"update_date\": \"2008-11-26\",\n",
      "  \"authors_parsed\": [\n",
      "    [\n",
      "      \"Bal\\u00e1zs\",\n",
      "      \"C.\",\n",
      "      \"\"\n",
      "    ],\n",
      "    [\n",
      "      \"Berger\",\n",
      "      \"E. L.\",\n",
      "      \"\"\n",
      "    ],\n",
      "    [\n",
      "      \"Nadolsky\",\n",
      "      \"P. M.\",\n",
      "      \"\"\n",
      "    ],\n",
      "    [\n",
      "      \"Yuan\",\n",
      "      \"C. -P.\",\n",
      "      \"\"\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"0704.0002\",\n",
      "  \"submitter\": \"Louis Theran\",\n",
      "  \"authors\": \"Ileana Streinu and Louis Theran\",\n",
      "  \"title\": \"Sparsity-certifying Graph Decompositions\",\n",
      "  \"comments\": \"To appear in Graphs and Combinatorics\",\n",
      "  \"journal-ref\": null,\n",
      "  \"doi\": null,\n",
      "  \"report-no\": null,\n",
      "  \"categories\": \"math.CO cs.CG\",\n",
      "  \"license\": \"http://arxiv.org/licenses/nonexclusive-distrib/1.0/\",\n",
      "  \"abstract\": \"  We describe a new algorithm, the $(k,\\\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the family of $(k,\\\\ell)$-sparse graphs and\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\nreceived increased attention in recent years. In particular, our colored\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\nalso present a new decomposition that certifies sparsity based on the\\n$(k,\\\\ell)$-pebble game with colors. Our work also exposes connections between\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\nWestermann and Hendrickson.\\n\",\n",
      "  \"versions\": [\n",
      "    {\n",
      "      \"version\": \"v1\",\n",
      "      \"created\": \"Sat, 31 Mar 2007 02:26:18 GMT\"\n",
      "    },\n",
      "    {\n",
      "      \"version\": \"v2\",\n",
      "      \"created\": \"Sat, 13 Dec 2008 17:26:00 GMT\"\n",
      "    }\n",
      "  ],\n",
      "  \"update_date\": \"2008-12-13\",\n",
      "  \"authors_parsed\": [\n",
      "    [\n",
      "      \"Streinu\",\n",
      "      \"Ileana\",\n",
      "      \"\"\n",
      "    ],\n",
      "    [\n",
      "      \"Theran\",\n",
      "      \"Louis\",\n",
      "      \"\"\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"0704.0003\",\n",
      "  \"submitter\": \"Hongjun Pan\",\n",
      "  \"authors\": \"Hongjun Pan\",\n",
      "  \"title\": \"The evolution of the Earth-Moon system based on the dark matter field\\n  fluid model\",\n",
      "  \"comments\": \"23 pages, 3 figures\",\n",
      "  \"journal-ref\": null,\n",
      "  \"doi\": null,\n",
      "  \"report-no\": null,\n",
      "  \"categories\": \"physics.gen-ph\",\n",
      "  \"license\": null,\n",
      "  \"abstract\": \"  The evolution of Earth-Moon system is described by the dark matter field\\nfluid model proposed in the Meeting of Division of Particle and Field 2004,\\nAmerican Physical Society. The current behavior of the Earth-Moon system agrees\\nwith this model very well and the general pattern of the evolution of the\\nMoon-Earth system described by this model agrees with geological and fossil\\nevidence. The closest distance of the Moon to Earth was about 259000 km at 4.5\\nbillion years ago, which is far beyond the Roche's limit. The result suggests\\nthat the tidal friction may not be the primary cause for the evolution of the\\nEarth-Moon system. The average dark matter field fluid constant derived from\\nEarth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts\\nthat the Mars's rotation is also slowing with the angular acceleration rate\\nabout -4.38 x 10^(-22) rad s^(-2).\\n\",\n",
      "  \"versions\": [\n",
      "    {\n",
      "      \"version\": \"v1\",\n",
      "      \"created\": \"Sun, 1 Apr 2007 20:46:54 GMT\"\n",
      "    },\n",
      "    {\n",
      "      \"version\": \"v2\",\n",
      "      \"created\": \"Sat, 8 Dec 2007 23:47:24 GMT\"\n",
      "    },\n",
      "    {\n",
      "      \"version\": \"v3\",\n",
      "      \"created\": \"Sun, 13 Jan 2008 00:36:28 GMT\"\n",
      "    }\n",
      "  ],\n",
      "  \"update_date\": \"2008-01-13\",\n",
      "  \"authors_parsed\": [\n",
      "    [\n",
      "      \"Pan\",\n",
      "      \"Hongjun\",\n",
      "      \"\"\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"0704.0004\",\n",
      "  \"submitter\": \"David Callan\",\n",
      "  \"authors\": \"David Callan\",\n",
      "  \"title\": \"A determinant of Stirling cycle numbers counts unlabeled acyclic\\n  single-source automata\",\n",
      "  \"comments\": \"11 pages\",\n",
      "  \"journal-ref\": null,\n",
      "  \"doi\": null,\n",
      "  \"report-no\": null,\n",
      "  \"categories\": \"math.CO\",\n",
      "  \"license\": null,\n",
      "  \"abstract\": \"  We show that a determinant of Stirling cycle numbers counts unlabeled acyclic\\nsingle-source automata. The proof involves a bijection from these automata to\\ncertain marked lattice paths and a sign-reversing involution to evaluate the\\ndeterminant.\\n\",\n",
      "  \"versions\": [\n",
      "    {\n",
      "      \"version\": \"v1\",\n",
      "      \"created\": \"Sat, 31 Mar 2007 03:16:14 GMT\"\n",
      "    }\n",
      "  ],\n",
      "  \"update_date\": \"2007-05-23\",\n",
      "  \"authors_parsed\": [\n",
      "    [\n",
      "      \"Callan\",\n",
      "      \"David\",\n",
      "      \"\"\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"0704.0005\",\n",
      "  \"submitter\": \"Alberto Torchinsky\",\n",
      "  \"authors\": \"Wael Abu-Shammala and Alberto Torchinsky\",\n",
      "  \"title\": \"From dyadic $\\\\Lambda_{\\\\alpha}$ to $\\\\Lambda_{\\\\alpha}$\",\n",
      "  \"comments\": null,\n",
      "  \"journal-ref\": \"Illinois J. Math. 52 (2008) no.2, 681-689\",\n",
      "  \"doi\": null,\n",
      "  \"report-no\": null,\n",
      "  \"categories\": \"math.CA math.FA\",\n",
      "  \"license\": null,\n",
      "  \"abstract\": \"  In this paper we show how to compute the $\\\\Lambda_{\\\\alpha}$ norm, $\\\\alpha\\\\ge\\n0$, using the dyadic grid. This result is a consequence of the description of\\nthe Hardy spaces $H^p(R^N)$ in terms of dyadic and special atoms.\\n\",\n",
      "  \"versions\": [\n",
      "    {\n",
      "      \"version\": \"v1\",\n",
      "      \"created\": \"Mon, 2 Apr 2007 18:09:58 GMT\"\n",
      "    }\n",
      "  ],\n",
      "  \"update_date\": \"2013-10-15\",\n",
      "  \"authors_parsed\": [\n",
      "    [\n",
      "      \"Abu-Shammala\",\n",
      "      \"Wael\",\n",
      "      \"\"\n",
      "    ],\n",
      "    [\n",
      "      \"Torchinsky\",\n",
      "      \"Alberto\",\n",
      "      \"\"\n",
      "    ]\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "    for i in range(5):  # 读取前5行\n",
    "        line = f.readline()\n",
    "        data = json.loads(line)\n",
    "        print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6e53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database arxiv_db already exists.\n",
      "User postgres already exists.\n",
      "Database setup complete. User postgres has access to arxiv_db.\n",
      "Tables created successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create db\n",
    "\n",
    "# 连接 PostgreSQL 创建数据库\n",
    "def create_database():\n",
    "    try:\n",
    "        # 连接到 PostgreSQL（不指定数据库名，默认连接到 postgres 数据库）\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=\"postgres\",\n",
    "            user=USER,\n",
    "            password=PASSWORD,\n",
    "            host=HOST,\n",
    "            port=PORT\n",
    "        )\n",
    "        conn.autocommit = True  # 设置为自动提交事务（创建数据库需要）\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # 创建数据库（如果不存在）\n",
    "        cur.execute(sql.SQL(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = %s\"), [DB_NAME])\n",
    "        if not cur.fetchone():\n",
    "            print(f\"Database {DB_NAME} does not exist. Creating it...\")\n",
    "            cur.execute(sql.SQL(\"CREATE DATABASE {}\").format(sql.Identifier(DB_NAME)))\n",
    "            print(f\"Database {DB_NAME} created successfully.\")\n",
    "        else:\n",
    "            print(f\"Database {DB_NAME} already exists.\")\n",
    "        \n",
    "        # 创建数据库用户\n",
    "        cur.execute(sql.SQL(\"SELECT 1 FROM pg_catalog.pg_roles WHERE rolname = %s\"), [DB_USER])\n",
    "        if not cur.fetchone():\n",
    "            print(f\"User {DB_USER} does not exist. Creating it...\")\n",
    "            cur.execute(sql.SQL(\"CREATE USER {} WITH PASSWORD %s\").format(sql.Identifier(DB_USER)), [DB_USER_PASSWORD])\n",
    "            print(f\"User {DB_USER} created successfully.\")\n",
    "        else:\n",
    "            print(f\"User {DB_USER} already exists.\")\n",
    "        \n",
    "        # 给用户权限\n",
    "        cur.execute(sql.SQL(\"GRANT ALL PRIVILEGES ON DATABASE {} TO {}\").format(sql.Identifier(DB_NAME), sql.Identifier(DB_USER)))\n",
    "\n",
    "        # 关闭连接\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "        print(f\"Database setup complete. User {DB_USER} has access to {DB_NAME}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# 连接到创建的数据库并创建表\n",
    "def create_tables():\n",
    "    try:\n",
    "        # 连接到已创建的数据库\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_USER_PASSWORD,\n",
    "            host=HOST,\n",
    "            port=PORT\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # 创建表\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS arxiv_papers (\n",
    "                id TEXT PRIMARY KEY,\n",
    "                submitter TEXT,\n",
    "                authors TEXT,\n",
    "                title TEXT,\n",
    "                comments TEXT,\n",
    "                journal_ref TEXT,\n",
    "                doi TEXT,\n",
    "                report_no TEXT,\n",
    "                categories TEXT,\n",
    "                license TEXT,\n",
    "                abstract TEXT,\n",
    "                update_date DATE\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS arxiv_versions (\n",
    "                paper_id TEXT REFERENCES arxiv_papers(id),\n",
    "                version TEXT,\n",
    "                created TIMESTAMP\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS arxiv_authors_parsed (\n",
    "                paper_id TEXT REFERENCES arxiv_papers(id),\n",
    "                last_name TEXT,\n",
    "                first_name TEXT,\n",
    "                middle_name TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "            ALTER TABLE arxiv_papers ADD COLUMN IF NOT EXISTS search_vector tsvector;\n",
    "        \"\"\")\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_arxiv_search_vector ON arxiv_papers USING GIN(search_vector);\n",
    "        \"\"\")\n",
    "\n",
    "        # 提交更改并关闭连接\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "        print(\"Tables created successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    create_database()\n",
    "    create_tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce48f578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Information:\n",
      "Name: arxiv_db\n",
      "Active Connections: 1\n",
      "Transactions Committed: 12\n",
      "Transactions Rolled Back: 1\n",
      "Blocks Read: 0\n",
      "Blocks Hit: 3373\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Reconnect to the database\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_USER_PASSWORD,\n",
    "        host=HOST,\n",
    "        port=PORT\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Query basic information about the database\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            datname AS database_name,\n",
    "            numbackends AS active_connections,\n",
    "            xact_commit AS transactions_committed,\n",
    "            xact_rollback AS transactions_rolled_back,\n",
    "            blks_read AS blocks_read,\n",
    "            blks_hit AS blocks_hit\n",
    "        FROM pg_stat_database\n",
    "        WHERE datname = %s;\n",
    "    \"\"\", (DB_NAME,))\n",
    "    \n",
    "    # Fetch and print the result\n",
    "    db_info = cur.fetchone()\n",
    "    print(\"Database Information:\")\n",
    "    print(f\"Name: {db_info[0]}\")\n",
    "    print(f\"Active Connections: {db_info[1]}\")\n",
    "    print(f\"Transactions Committed: {db_info[2]}\")\n",
    "    print(f\"Transactions Rolled Back: {db_info[3]}\")\n",
    "    print(f\"Blocks Read: {db_info[4]}\")\n",
    "    print(f\"Blocks Hit: {db_info[5]}\")\n",
    "\n",
    "    # Close the connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving database information: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b074619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully inserted into the database.\n"
     ]
    }
   ],
   "source": [
    "#insert json into db\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    # Reconnect to the database\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_USER_PASSWORD,\n",
    "        host=HOST,\n",
    "        port=PORT\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Open the JSON file\n",
    "    with open('arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "        # Get the total number of lines in the file\n",
    "        total_lines = sum(1 for _ in open('arxiv-metadata-oai-snapshot.json', 'r'))\n",
    "        f.seek(0)  # Reset file pointer to the beginning\n",
    "\n",
    "        batch_size = 1000\n",
    "        batch = []\n",
    "\n",
    "        # Use tqdm to display the progress bar\n",
    "        with tqdm(total=total_lines, desc=\"Importing JSON data\") as pbar:\n",
    "            for line in f:\n",
    "                # Parse each line as JSON\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # Extract relevant fields\n",
    "                paper_id = data.get('id')\n",
    "                submitter = data.get('submitter')\n",
    "                authors = data.get('authors')\n",
    "                title = data.get('title')\n",
    "                comments = data.get('comments')\n",
    "                journal_ref = data.get('journal-ref')\n",
    "                doi = data.get('doi')\n",
    "                report_no = data.get('report-no')\n",
    "                categories = data.get('categories')\n",
    "                license = data.get('license')\n",
    "                abstract = data.get('abstract')\n",
    "                update_date = data.get('update_date')\n",
    "\n",
    "                # Convert update_date to a proper date format\n",
    "                if update_date:\n",
    "                    update_date = datetime.strptime(update_date, '%Y-%m-%d').date()\n",
    "\n",
    "                # Add to batch\n",
    "                batch.append((\n",
    "                    paper_id, submitter, authors, title, comments, journal_ref, doi,\n",
    "                    report_no, categories, license, abstract, update_date\n",
    "                ))\n",
    "\n",
    "                # Insert batch into the database\n",
    "                if len(batch) >= batch_size:\n",
    "                    cur.executemany(\"\"\"\n",
    "                        INSERT INTO arxiv_papers (\n",
    "                            id, submitter, authors, title, comments, journal_ref, doi,\n",
    "                            report_no, categories, license, abstract, update_date\n",
    "                        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                        ON CONFLICT (id) DO NOTHING;\n",
    "                    \"\"\", batch)\n",
    "                    conn.commit()\n",
    "                    batch = []\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Insert any remaining records\n",
    "            if batch:\n",
    "                cur.executemany(\"\"\"\n",
    "                    INSERT INTO arxiv_papers (\n",
    "                        id, submitter, authors, title, comments, journal_ref, doi,\n",
    "                        report_no, categories, license, abstract, update_date\n",
    "                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                    ON CONFLICT (id) DO NOTHING;\n",
    "                \"\"\", batch)\n",
    "                conn.commit()\n",
    "\n",
    "    print(\"Data successfully inserted into the database.\")\n",
    "\n",
    "    # Close the connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting data into the database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9323d1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Databases:\n",
      "- postgres\n",
      "- arxiv_db\n",
      "==================================================\n",
      "数据库基本信息:\n",
      "数据库名称: arxiv_db\n",
      "数据库大小: 3599 MB\n",
      "活动连接数: 2\n",
      "已提交事务: 1605\n",
      "回滚事务: 5\n",
      "磁盘读取块: 451\n",
      "缓存命中块: 53733\n",
      "\n",
      "==================================================\n",
      "表信息:\n",
      "\n",
      "表名: arxiv_authors_parsed\n",
      "行数: 0\n",
      "总大小: 8192 bytes\n",
      "表大小: 8192 bytes\n",
      "索引大小: 0 bytes\n",
      "\n",
      "表名: arxiv_versions\n",
      "行数: 0\n",
      "总大小: 8192 bytes\n",
      "表大小: 8192 bytes\n",
      "索引大小: 0 bytes\n",
      "\n",
      "表名: arxiv_papers\n",
      "行数: 0\n",
      "总大小: 3591 MB\n",
      "表大小: 3500 MB\n",
      "索引大小: 91 MB\n",
      "\n",
      "==================================================\n",
      "表结构信息:\n",
      "\n",
      "表 arxiv_authors_parsed 的结构:\n",
      "列名: paper_id\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "列名: last_name\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "列名: first_name\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "列名: middle_name\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "\n",
      "表 arxiv_versions 的结构:\n",
      "列名: created\n",
      "数据类型: timestamp without time zone\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "列名: paper_id\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "列名: version\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "\n",
      "表 arxiv_papers 的结构:\n",
      "列名: update_date\n",
      "数据类型: date\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "列名: search_vector\n",
      "数据类型: tsvector\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "列名: authors\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "列名: title\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "列名: comments\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "列名: journal_ref\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "列名: id\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: NO\n",
      "------------------------------\n",
      "列名: report_no\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "列名: categories\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "列名: license\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "列名: abstract\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "列名: doi\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "列名: submitter\n",
      "数据类型: text\n",
      "最大长度: None\n",
      "默认值: None\n",
      "允许空值: YES\n",
      "------------------------------\n",
      "\n",
      "==================================================\n",
      "索引信息:\n",
      "\n",
      "模式: public\n",
      "表名: arxiv_papers\n",
      "索引名: arxiv_papers_pkey\n",
      "索引定义: CREATE UNIQUE INDEX arxiv_papers_pkey ON public.arxiv_papers USING btree (id)\n",
      "\n",
      "模式: public\n",
      "表名: arxiv_papers\n",
      "索引名: idx_arxiv_search_vector\n",
      "索引定义: CREATE INDEX idx_arxiv_search_vector ON public.arxiv_papers USING gin (search_vector)\n"
     ]
    }
   ],
   "source": [
    "# db info\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "# 数据库连接参数\n",
    "HOST = 'localhost'  # 数据库主机\n",
    "PORT = '5432'  # PostgreSQL 默认端口\n",
    "USER = 'postgres'  # 用于创建数据库的管理员用户名\n",
    "PASSWORD = '123456'  # 管理员密码\n",
    "DB_NAME = 'arxiv_db'  # 要创建的数据库名称\n",
    "DB_USER = 'postgres'  # 新用户\n",
    "DB_USER_PASSWORD = '123456'  # 新用户密码\n",
    "try:\n",
    "    # Connect to the PostgreSQL server\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"postgres\",  # 连接到默认的 postgres 数据库\n",
    "        user=USER,\n",
    "        password=PASSWORD,\n",
    "        host=HOST,\n",
    "        port=PORT\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Query all databases\n",
    "    cur.execute(\"SELECT datname FROM pg_database WHERE datistemplate = false;\")\n",
    "    databases = cur.fetchall()\n",
    "\n",
    "    # Print the list of databases\n",
    "    print(\"Available Databases:\")\n",
    "    for db in databases:\n",
    "        print(f\"- {db[0]}\")\n",
    "\n",
    "    # Close the connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error querying databases: {e}\")\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_USER_PASSWORD,\n",
    "        host=HOST,\n",
    "        port=PORT\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # 1. 基本数据库信息\n",
    "    print(\"=\"*50)\n",
    "    print(\"数据库基本信息:\")\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            datname AS database_name,\n",
    "            pg_size_pretty(pg_database_size(datname)) AS database_size,\n",
    "            numbackends AS active_connections,\n",
    "            xact_commit AS transactions_committed,\n",
    "            xact_rollback AS transactions_rolled_back,\n",
    "            blks_read AS blocks_read,\n",
    "            blks_hit AS blocks_hit\n",
    "        FROM pg_stat_database\n",
    "        WHERE datname = %s;\n",
    "    \"\"\", (DB_NAME,))\n",
    "    db_info = cur.fetchone()\n",
    "    print(f\"数据库名称: {db_info[0]}\")\n",
    "    print(f\"数据库大小: {db_info[1]}\")\n",
    "    print(f\"活动连接数: {db_info[2]}\")\n",
    "    print(f\"已提交事务: {db_info[3]}\")\n",
    "    print(f\"回滚事务: {db_info[4]}\")\n",
    "    print(f\"磁盘读取块: {db_info[5]}\")\n",
    "    print(f\"缓存命中块: {db_info[6]}\")\n",
    "    # 2. 表信息\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    print(\"表信息:\")\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT \n",
    "            relname AS table_name,\n",
    "            n_live_tup AS row_count,\n",
    "            pg_size_pretty(pg_total_relation_size(quote_ident(relname))) AS total_size,\n",
    "            pg_size_pretty(pg_table_size(quote_ident(relname))) AS table_size,\n",
    "            pg_size_pretty(pg_indexes_size(quote_ident(relname))) AS index_size\n",
    "        FROM pg_stat_user_tables\n",
    "        ORDER BY n_live_tup DESC;\n",
    "    \"\"\")\n",
    "    tables_info = cur.fetchall()\n",
    "    for table in tables_info:\n",
    "        print(f\"\\n表名: {table[0]}\")\n",
    "        print(f\"行数: {table[1]}\")\n",
    "        print(f\"总大小: {table[2]}\")\n",
    "        print(f\"表大小: {table[3]}\")\n",
    "        print(f\"索引大小: {table[4]}\")\n",
    "    # 3. 表结构信息\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    print(\"表结构信息:\")\n",
    "    for table_info in tables_info:\n",
    "        table_name = table_info[0]\n",
    "        print(f\"\\n表 {table_name} 的结构:\")\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT \n",
    "                column_name,\n",
    "                data_type,\n",
    "                character_maximum_length,\n",
    "                column_default,\n",
    "                is_nullable\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_name = %s;\n",
    "        \"\"\", (table_name,))\n",
    "        columns = cur.fetchall()\n",
    "        for col in columns:\n",
    "            print(f\"列名: {col[0]}\")\n",
    "            print(f\"数据类型: {col[1]}\")\n",
    "            print(f\"最大长度: {col[2]}\")\n",
    "            print(f\"默认值: {col[3]}\")\n",
    "            print(f\"允许空值: {col[4]}\")\n",
    "            print(\"-\"*30)\n",
    "\n",
    "    # 4. 索引信息\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    print(\"索引信息:\")\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT\n",
    "            schemaname,\n",
    "            tablename,\n",
    "            indexname,\n",
    "            indexdef\n",
    "        FROM pg_indexes\n",
    "        WHERE schemaname = 'public'\n",
    "        ORDER BY tablename, indexname;\n",
    "    \"\"\")\n",
    "    indexes = cur.fetchall()\n",
    "    for idx in indexes:\n",
    "        print(f\"\\n模式: {idx[0]}\")\n",
    "        print(f\"表名: {idx[1]}\")\n",
    "        print(f\"索引名: {idx[2]}\")\n",
    "        print(f\"索引定义: {idx[3]}\")\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting data into the database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d36bfb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真实行数统计：\n",
      "表 arxiv_authors_parsed 实际行数: 0\n",
      "表 arxiv_versions 实际行数: 0\n",
      "表 arxiv_papers 实际行数: 2725378\n"
     ]
    }
   ],
   "source": [
    "# 数据库连接参数\n",
    "HOST = 'localhost'  # 数据库主机\n",
    "PORT = '5432'  # PostgreSQL 默认端口\n",
    "USER = 'postgres'  # 用于创建数据库的管理员用户名\n",
    "PASSWORD = '123456'  # 管理员密码\n",
    "DB_NAME = 'arxiv_db'  # 要创建的数据库名称\n",
    "DB_USER = 'postgres'  # 新用户\n",
    "DB_USER_PASSWORD = '123456'  # 新用户密码\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_USER_PASSWORD,\n",
    "        host=HOST,\n",
    "        port=PORT\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    print(\"真实行数统计：\")\n",
    "    for table_info in tables_info:\n",
    "        table_name = table_info[0]\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {table_name};\")\n",
    "        real_count = cur.fetchone()[0]\n",
    "        print(f\"表 {table_name} 实际行数: {real_count}\")\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"Error counting rows: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b503577b",
   "metadata": {},
   "source": [
    "######ANalyse abstract###########\n",
    "# Aim: classification\n",
    "# 1 Read the abstract and catagory from db,  \n",
    "# 2 \n",
    "\n",
    "use the abstract to classify all articles, since the catagory tag in the db already specify the 1st level of tags, i need to use openAI type model to do further claasification based on the abstract. there is a local model api that can be called using localhost:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e4cfd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb76a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee92d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a94c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verterize into LLM\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
